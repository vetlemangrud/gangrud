{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu0eI83UnDKR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"\"\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 128 #Actual frames = 1440. must be power of 2\n",
        "IMAGE_CHANNELS = 3\n",
        "LATENT_VECTOR_SIZE = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_examples(model, title=None, big=False):\n",
        "    \"\"\"Display examples of generated images\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The generator model to use\n",
        "        title (str, optional): The title of the display. Defaults to None.\n",
        "        big (bool, optional): If false, display 4x1 examples, if true, display 4x3. Defaults to False.\n",
        "    \"\"\"\n",
        "    latent_space_samples = torch.randn((batch_size, LATENT_VECTOR_SIZE))\n",
        "    generated_samples = model(latent_space_samples)\n",
        "\n",
        "    generated_samples = generated_samples.detach()\n",
        "    fig = plt.figure()\n",
        "    if title:\n",
        "        fig.text(0.5, 1.0 if big else 0.7, title, ha='center', va='center')\n",
        "    for plot_idx, dataset_idx  in enumerate(torch.randint(len(generated_samples), (12 if big else 4,))):\n",
        "        sub = plt.subplot(3 if big else 1, 4, plot_idx+1)\n",
        "        plt.tight_layout()\n",
        "        sub.axis('off')\n",
        "        image = generated_samples[dataset_idx.item()].permute(1,2,0)\n",
        "        plt.imshow(image *0.5 + 0.5)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv_D5273nzv9"
      },
      "source": [
        "# Preparing training data\n",
        "Here I have two datasets. The reason is that I initially wanted to generate images of myself, like a good egoistic human would. The problem was that the reference images are very similar, so the generator got good at generating the background and my hair. However, the face was just a blob, destroing the point. Also, when testing on the flower dataset, I noticed that the training went much faster. This is probably a result of better optimizations and transformations when reading the images. I could look closer into optimizing the `VetleDataset`, but the flowers were more interesting to look at anyway. I left the old dataset because it should work given training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VetleDataset(Dataset):\n",
        "\n",
        "    def __init__(self, sample_count, image_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            sample_count (int): How many samples\n",
        "            image_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.sample_count = sample_count\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sample_count\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "            \n",
        "        frameno = str(idx+1).zfill(4)\n",
        "        filename = \"frame\" + frameno + \".png\"\n",
        "\n",
        "        img_name = os.path.join(self.image_dir,\n",
        "                                filename)\n",
        "        image = torchvision.io.read_image(img_name).float()\n",
        "        \n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Return image with a label. We won't use it, but it seems like tensorflow expects it\n",
        "        return [image,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vetleTransform = transforms.Compose([ transforms.Resize(IMAGE_SIZE), transforms.Normalize(128,128)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgKQUWtYnqw7"
      },
      "outputs": [],
      "source": [
        "vetleDataset = VetleDataset(2905, \"assets/frames/\", transform=vetleTransform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flowerTransform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),  \n",
        "    transforms.RandomCrop(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flowerDataset = torchvision.datasets.Flowers102(root=\"assets\", download=True, transform=flowerTransform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SELECTED_DATASET = flowerDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNISr204ogN6"
      },
      "source": [
        "Plot training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "iVeBOtd9oiRn",
        "outputId": "5beb42c8-b95b-4379-e3e6-a1d3b9007785"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "for plot_idx, dataset_idx  in enumerate(torch.randint(len(SELECTED_DATASET), (4,))):\n",
        "    sub = plt.subplot(1, 4, plot_idx+1)\n",
        "    plt.tight_layout()\n",
        "    sub.axis('off')\n",
        "    image = SELECTED_DATASET[dataset_idx.item()][0].permute(1,2,0)\n",
        "    plt.imshow(image * 0.5 + 0.5)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Bf5AZDooHy"
      },
      "source": [
        "Create data loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRWGyhu6op8w"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    SELECTED_DATASET, batch_size=batch_size, shuffle=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHZgCgetoxJR"
      },
      "source": [
        "# The models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82lyWxl0o0Ju"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cT33zUIo1m6"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=IMAGE_CHANNELS, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "\n",
        "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "            nn.Sigmoid() \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output.view(x.size(0),1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDaab3-Po7W5"
      },
      "outputs": [],
      "source": [
        "discriminator = Discriminator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41wQXoXEo9zc"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LClQcFWtpAY0"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "       \n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=LATENT_VECTOR_SIZE, out_channels=1024, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0),LATENT_VECTOR_SIZE,1,1)\n",
        "        return self.model(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI_hyXrgpIex"
      },
      "outputs": [],
      "source": [
        "generator = Generator()\n",
        "show_examples(generator, title=\"Initial\", big=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYlsn_DfpLLt"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPf7VoGepVGz"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uykVkGTvpPSp"
      },
      "outputs": [],
      "source": [
        "lr = 0.0002\n",
        "num_epochs = 300\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5,0.999))\n",
        "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5,0.999))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxVkV_7WsXgs"
      },
      "source": [
        "Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fZFI3KGsXMj",
        "outputId": "292609cd-3434-403d-f028-6ddfd65afa54"
      },
      "outputs": [],
      "source": [
        "timeline = {\n",
        "    \"loss_discriminator\":[],\n",
        "    \"loss_generator\":[],\n",
        "}\n",
        "for epoch in range(num_epochs):\n",
        "    print(epoch, end=\" \")\n",
        "    for n, (real_samples, _) in enumerate(train_loader):\n",
        "        # Data for training the discriminator\n",
        "        real_samples_labels = torch.ones((real_samples.shape[0], 1))\n",
        "        latent_space_samples = torch.randn((batch_size, LATENT_VECTOR_SIZE))\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        generated_samples_labels = torch.zeros((batch_size, 1))\n",
        "        all_samples = torch.cat((real_samples, generated_samples))\n",
        "        all_samples_labels = torch.cat(\n",
        "            (real_samples_labels, generated_samples_labels)\n",
        "        )\n",
        "\n",
        "        # Training the discriminator\n",
        "        discriminator.zero_grad()\n",
        "        discriminator_loss = 0\n",
        "        ## First only real samples\n",
        "        output_discriminator_real = discriminator(real_samples)\n",
        "        loss_discriminator_real = loss_function(\n",
        "            output_discriminator_real, real_samples_labels)\n",
        "        loss_discriminator_real.backward()\n",
        "        discriminator_loss += loss_discriminator_real.detach()\n",
        "        \n",
        "        ## Then only generated samples\n",
        "        output_discriminator_gen = discriminator(generated_samples)\n",
        "        loss_discriminator_gen = loss_function(\n",
        "            output_discriminator_gen, generated_samples_labels)\n",
        "        loss_discriminator_gen.backward()\n",
        "        \n",
        "        optimizer_discriminator.step()\n",
        "        discriminator_loss += loss_discriminator_gen.detach()\n",
        "\n",
        "        # Data for training the generator\n",
        "        latent_space_samples = torch.randn((batch_size, LATENT_VECTOR_SIZE))\n",
        "        real_labels = torch.ones((batch_size, 1))\n",
        "\n",
        "        # Training the generator\n",
        "        generator.zero_grad()\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        output_discriminator_generated = discriminator(generated_samples)\n",
        "        loss_generator = loss_function(\n",
        "            output_discriminator_generated, real_labels\n",
        "        )\n",
        "        loss_generator.backward()\n",
        "        optimizer_generator.step()\n",
        "\n",
        "        # Show loss\n",
        "        if n == len(train_loader) - 1:\n",
        "            timeline[\"loss_discriminator\"].append(discriminator_loss)\n",
        "            timeline[\"loss_generator\"].append(loss_generator.detach())\n",
        "            if epoch % 10 == 0 or epoch == num_epochs -1:\n",
        "                print(\"\")\n",
        "                print(f\"Epoch: {epoch} Loss D.: {discriminator_loss}\")\n",
        "                print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")\n",
        "                show_examples(generator, title=f\"Epoch {epoch}\")\n",
        "           "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgDLBugXsfc5"
      },
      "source": [
        "# Results\n",
        "## Example images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "TCNIwh3WsjJo",
        "outputId": "f8b6762d-b9bf-4d8c-9334-3f45d31639ac"
      },
      "outputs": [],
      "source": [
        "show_examples(generator, title=\"End Results\", big=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A look at lerping\n",
        "I was concerned that the generator might overfit to the reference images. A perfect way to trick the discriminator is to \"learn\" one or a few of these images. A way to detect overfitting in GANs is to smoothly lerp between two different latent vectors and look for sudden \"jumps\" in the output image. A jump like this can signify that the generator has learnt a few examples, and is reciting them based on some value in the input noise.\n",
        "\n",
        "If we see that the resulting flowers gradually change from one to another, we can guess that the generator actually generates meaningful properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample two random points in latent space\n",
        "latent_space_endpoints = torch.randn((2, LATENT_VECTOR_SIZE))\n",
        "\n",
        "# Generate a list of equally spaced points between the two endpoints\n",
        "STEPS = 10\n",
        "latent_space_samples = torch.empty(STEPS, LATENT_VECTOR_SIZE)\n",
        "for i in range(STEPS):\n",
        "    latent_space_samples[i] = torch.lerp(latent_space_endpoints[0], latent_space_endpoints[1],float(i)/float(STEPS+10))\n",
        "\n",
        "# Generate images from the list of points\n",
        "generated_samples = generator(latent_space_samples)\n",
        "generated_samples = generated_samples.detach()\n",
        "\n",
        "# Display the images in order\n",
        "fig, axes = plt.subplots(1, STEPS, figsize=(10,2))\n",
        "fig.text(0.5, 0.8, \"Looking at the lerping betweent two flowers\", ha='center', va='center')\n",
        "\n",
        "for i  in range(STEPS):\n",
        "    sub = plt.subplot(1, STEPS, i+1)\n",
        "    sub.axis('off')\n",
        "    image = generated_samples[i].permute(1,2,0)\n",
        "    axes[i].imshow(image * 0.5 + 0.5)\n",
        "plt.tight_layout(pad=0.5)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot = plt.figure()\n",
        "plt.plot(range(0, num_epochs), timeline[\"loss_discriminator\"], label=\"Discriminator loss\")\n",
        "plt.plot(range(0, num_epochs), timeline[\"loss_generator\"], label=\"Generator loss\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's a dumb and simple saving system. To save the generator, set `SAVE` to true, and give it a `NAME`. To load a previously saved generator, set `SAVE` to false, and input the `NAME` of the model to load."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAVE = False\n",
        "NAME = \"Flowers300\"\n",
        "path = f\"saved/{NAME}.gangrud\"\n",
        "if SAVE:\n",
        "    torch.save(generator.state_dict(), path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saved_model = Generator()\n",
        "saved_model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_examples(saved_model, title=\"Saved Model\", big=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
